{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Effective Approaches to Attention-based Neural Machine Translation</center>\n",
    "<center> <strong> Submitted By : Aniruddha P. Deshpande (20161058) </strong> </center>\n",
    "<center> <strong> Reference : </strong>\n",
    "<a href=\"https://github.com/ShichaoJin/Neural-Machine-Translation\" title=\"Neural Machine Translation implemented in PyTorch\">Neural Machine Translation implemented in PyTorch</a>  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Helper Functions for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "\n",
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))\n",
    "\n",
    "\n",
    "def validate_language_params(l):\n",
    "    is_missing = (not os.path.exists('../data/attention_params_{}'.format(l))\n",
    "                  or not os.path.exists('../data/decoder_params_{}'.format(l))\n",
    "                  or not os.path.exists('../data/encoder_params_{}'.format(l)))\n",
    "\n",
    "    if is_missing:\n",
    "        print(\"Model params for language '{}' do not exist in the data directory. Please train a new model for this language.\".format(l))\n",
    "        exit(1)\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Language Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    sos_token = 0\n",
    "    eos_token = 1\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: '<SOS>', 1: '<EOS>', '<UNK>': 2}\n",
    "        self.n_words = len(self.index2word)\n",
    "\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 20\n",
    "\n",
    "# Data Extraction\n",
    "\n",
    "def filter_pair(p):\n",
    "    is_good_length = len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length\n",
    "    return is_good_length\n",
    "\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "\n",
    "    # Read and filter sentences\n",
    "    input_lang, output_lang, pairs = read_languages()\n",
    "    pairs = filter_pairs(pairs)\n",
    "\n",
    "    # Index words\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "def read_languages():\n",
    "\n",
    "    # Read and parse the text file\n",
    "    doc_eng = open('../../Data/enghin/train.en', encoding='utf-8').read()\n",
    "    doc_hin = open('../../Data/enghin/train.hi', encoding='utf-8').read()\n",
    "    lines_eng = doc_eng.strip().split('\\n')\n",
    "    lines_hin = doc_hin.strip().split('\\n')\n",
    "\n",
    "    # Transform the data and initialize language instances\n",
    "    pairs = []\n",
    "    for i in range(0,len(lines_eng)):\n",
    "        curr_pair = [normalize_string(lines_eng[i]), lines_hin[i]]\n",
    "        pairs.append(curr_pair)\n",
    "    \n",
    "    input_lang = Language('eng')\n",
    "    output_lang = Language('hin')\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "#Data Transformation\n",
    "\n",
    "\n",
    "# Returns a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(1)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    var = var.cuda()\n",
    "    return var\n",
    "\n",
    "\n",
    "def variables_from_pair(pair, input_lang, output_lang):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return input_variable, target_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Global Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention nn module that is responsible for computing the alignment scores.\"\"\"\n",
    "\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Define layers\n",
    "        if self.method == 'general':\n",
    "            self.attention = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attention = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, self.hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"Attend all encoder inputs conditioned on the previous hidden state of the decoder.\n",
    "        \n",
    "        After creating variables to store the attention energies, calculate their \n",
    "        values for each encoder output and return the normalized values.\n",
    "        \n",
    "        Args:\n",
    "            hidden: decoder hidden output used for condition\n",
    "            encoder_outputs: list of encoder outputs\n",
    "            \n",
    "        Returns:\n",
    "             Normalized (0..1) energy values, re-sized to 1 x 1 x seq_len\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = len(encoder_outputs)\n",
    "        energies = Variable(torch.zeros(seq_len)).cuda()\n",
    "        for i in range(seq_len):\n",
    "            energies[i] = self._score(hidden, encoder_outputs[i])\n",
    "        return F.softmax(energies).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    def _score(self, hidden, encoder_output):\n",
    "        \"\"\"Calculate the relevance of a particular encoder output in respect to the decoder hidden.\"\"\"\n",
    "\n",
    "        if self.method == 'dot':\n",
    "            energy = torch.dot(hidden.view(-1), encoder_output.view(-1))\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attention(encoder_output)\n",
    "            energy = torch.dot(hidden.view(-1), energy.view(-1))\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attention(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = torch.dot(self.other.view(-1), energy.view(-1))\n",
    "        return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"Recurrent neural network that encodes a given input sequence.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "\n",
    "    def forward(self, word_inputs, hidden):\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoderRNN(nn.Module):\n",
    "    \"\"\"Recurrent neural network that makes use of gated recurrent units to translate encoded inputs using attention.\"\"\"\n",
    "\n",
    "    def __init__(self, attention_model, hidden_size, output_size, n_layers=1, dropout_p=.1):\n",
    "        super(AttentionDecoderRNN, self).__init__()\n",
    "        self.attention_model = attention_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "        # Choose attention model\n",
    "        if attention_model is not None:\n",
    "            self.attention = Attention(attention_model, hidden_size)\n",
    "\n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        \"\"\"Run forward propagation one step at a time.\n",
    "        \n",
    "        Get the embedding of the current input word (last output word) [s = 1 x batch_size x seq_len]\n",
    "        then combine them with the previous context. Use this as input and run through the RNN. Next,\n",
    "        calculate the attention from the current RNN state and all encoder outputs. The final output\n",
    "        is the next word prediction using the RNN hidden state and context vector.\n",
    "        \n",
    "        Args:\n",
    "            word_input: torch Variable representing the word input constituent\n",
    "            last_context: torch Variable representing the previous context\n",
    "            last_hidden: torch Variable representing the previous hidden state output\n",
    "            encoder_outputs: torch Variable containing the encoder output values\n",
    "            \n",
    "        Return:\n",
    "            output: torch Variable representing the predicted word constituent \n",
    "            context: torch Variable representing the context value\n",
    "            hidden: torch Variable representing the hidden state of the RNN\n",
    "            attention_weights: torch Variable retrieved from the attention model\n",
    "        \"\"\"\n",
    "\n",
    "        # Run through RNN\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1)\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention\n",
    "        attention_weights = self.attention(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attention_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "\n",
    "        # Predict output\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)))\n",
    "        return output, context, hidden, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = .5\n",
    "clip = 5.\n",
    "\n",
    "\n",
    "def train(input_var, target_var, encoder, decoder, encoder_opt, decoder_opt, criterion):\n",
    "    # Initialize optimizers and loss\n",
    "    encoder_opt.zero_grad()\n",
    "    decoder_opt.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # Get input and target seq lengths\n",
    "    target_length = target_var.size()[0]\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n",
    "\n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([0]))\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_context = decoder_context.cuda()\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Scheduled sampling\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        # Feed target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input,\n",
    "                                                                                         decoder_context,\n",
    "                                                                                         decoder_hidden,\n",
    "                                                                                         encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_var[di])\n",
    "            decoder_input = target_var[di]\n",
    "    else:\n",
    "        # Use previous prediction as next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input,\n",
    "                                                                                         decoder_context,\n",
    "                                                                                         decoder_hidden,\n",
    "                                                                                         encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_var[di])\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            ni = topi[0][0]\n",
    "\n",
    "            decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "            decoder_input = decoder_input.cuda()\n",
    "\n",
    "            if ni == 1:\n",
    "                break\n",
    "\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_opt.step()\n",
    "    decoder_opt.step()\n",
    "    \n",
    "    return loss.data / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 Running Training Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/aniruddha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/aniruddha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:56: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/aniruddha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:57: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6m 50s (- 677m 23s) (1000 1%) 6.0246\n",
      "13m 9s (- 645m 0s) (2000 2%) 5.9256\n",
      "19m 58s (- 646m 1s) (3000 3%) 5.8807\n",
      "26m 31s (- 636m 26s) (4000 4%) 5.7545\n",
      "35m 18s (- 670m 44s) (5000 5%) 5.8105\n",
      "44m 7s (- 691m 24s) (6000 6%) 5.7838\n",
      "51m 49s (- 688m 32s) (7000 7%) 5.6374\n",
      "59m 40s (- 686m 14s) (8000 8%) 5.6474\n",
      "68m 2s (- 687m 55s) (9000 9%) 5.5283\n",
      "77m 18s (- 695m 42s) (10000 10%) 5.6007\n",
      "87m 38s (- 709m 5s) (11000 11%) 5.5420\n",
      "98m 53s (- 725m 15s) (12000 12%) 5.5947\n",
      "109m 32s (- 733m 5s) (13000 13%) 5.4621\n",
      "116m 24s (- 715m 2s) (14000 14%) 5.4406\n",
      "123m 27s (- 699m 34s) (15000 15%) 5.3307\n",
      "130m 47s (- 686m 38s) (16000 16%) 5.4215\n",
      "137m 57s (- 673m 31s) (17000 17%) 5.3096\n",
      "145m 46s (- 664m 3s) (18000 18%) 5.2336\n",
      "152m 34s (- 650m 26s) (19000 19%) 5.1815\n",
      "159m 41s (- 638m 45s) (20000 20%) 5.2348\n",
      "166m 44s (- 627m 16s) (21000 21%) 5.1911\n",
      "173m 39s (- 615m 41s) (22000 22%) 5.1468\n",
      "180m 37s (- 604m 42s) (23000 23%) 5.1186\n",
      "187m 35s (- 594m 1s) (24000 24%) 5.1159\n",
      "194m 27s (- 583m 22s) (25000 25%) 5.0986\n",
      "201m 24s (- 573m 14s) (26000 26%) 5.1179\n",
      "208m 28s (- 563m 38s) (27000 27%) 5.0496\n",
      "215m 28s (- 554m 3s) (28000 28%) 5.0463\n",
      "222m 39s (- 545m 6s) (29000 28%) 5.0237\n",
      "229m 39s (- 535m 52s) (30000 30%) 5.0128\n",
      "236m 40s (- 526m 47s) (31000 31%) 4.9121\n",
      "243m 49s (- 518m 7s) (32000 32%) 4.9035\n",
      "253m 38s (- 514m 58s) (33000 33%) 5.0013\n",
      "265m 7s (- 514m 40s) (34000 34%) 4.8504\n",
      "276m 49s (- 514m 6s) (35000 35%) 4.8899\n",
      "288m 42s (- 513m 15s) (36000 36%) 4.8818\n",
      "300m 29s (- 511m 38s) (37000 37%) 4.8120\n",
      "309m 48s (- 505m 29s) (38000 38%) 4.8534\n",
      "317m 15s (- 496m 13s) (39000 39%) 4.8584\n",
      "328m 38s (- 492m 57s) (40000 40%) 4.8301\n",
      "340m 13s (- 489m 35s) (41000 41%) 4.7203\n",
      "351m 42s (- 485m 41s) (42000 42%) 4.7109\n",
      "363m 13s (- 481m 29s) (43000 43%) 4.7853\n",
      "374m 54s (- 477m 8s) (44000 44%) 4.7516\n",
      "386m 24s (- 472m 17s) (45000 45%) 4.6479\n",
      "397m 57s (- 467m 10s) (46000 46%) 4.7583\n",
      "408m 10s (- 460m 16s) (47000 47%) 4.6128\n",
      "415m 7s (- 449m 43s) (48000 48%) 4.7012\n",
      "422m 17s (- 439m 31s) (49000 49%) 4.6153\n",
      "429m 26s (- 429m 26s) (50000 50%) 4.6585\n",
      "436m 42s (- 419m 35s) (51000 51%) 4.5433\n",
      "444m 0s (- 409m 50s) (52000 52%) 4.5996\n",
      "451m 19s (- 400m 14s) (53000 53%) 4.5686\n",
      "458m 48s (- 390m 50s) (54000 54%) 4.5925\n",
      "466m 5s (- 381m 20s) (55000 55%) 4.5015\n",
      "473m 19s (- 371m 53s) (56000 56%) 4.5178\n",
      "480m 36s (- 362m 34s) (57000 56%) 4.4428\n",
      "487m 54s (- 353m 18s) (58000 57%) 4.4952\n",
      "495m 8s (- 344m 5s) (59000 59%) 4.4785\n",
      "502m 26s (- 334m 57s) (60000 60%) 4.4656\n",
      "509m 41s (- 325m 51s) (61000 61%) 4.3800\n",
      "517m 7s (- 316m 57s) (62000 62%) 4.3822\n",
      "524m 53s (- 308m 16s) (63000 63%) 4.4261\n",
      "532m 12s (- 299m 21s) (64000 64%) 4.4164\n",
      "539m 26s (- 290m 28s) (65000 65%) 4.3866\n",
      "546m 45s (- 281m 39s) (66000 66%) 4.3945\n",
      "553m 58s (- 272m 51s) (67000 67%) 4.3727\n",
      "561m 23s (- 264m 10s) (68000 68%) 4.3729\n",
      "569m 13s (- 255m 44s) (69000 69%) 4.4230\n",
      "576m 27s (- 247m 3s) (70000 70%) 4.3583\n",
      "583m 47s (- 238m 26s) (71000 71%) 4.2683\n",
      "591m 4s (- 229m 51s) (72000 72%) 4.2997\n",
      "598m 10s (- 221m 14s) (73000 73%) 4.2511\n",
      "605m 26s (- 212m 43s) (74000 74%) 4.3425\n",
      "612m 47s (- 204m 15s) (75000 75%) 4.2829\n",
      "620m 0s (- 195m 47s) (76000 76%) 4.2726\n",
      "627m 20s (- 187m 23s) (77000 77%) 4.2259\n",
      "634m 33s (- 178m 58s) (78000 78%) 4.2245\n",
      "642m 1s (- 170m 39s) (79000 79%) 4.1796\n",
      "649m 21s (- 162m 20s) (80000 80%) 4.2043\n",
      "656m 59s (- 154m 6s) (81000 81%) 4.1821\n",
      "664m 15s (- 145m 48s) (82000 82%) 4.1745\n",
      "671m 39s (- 137m 34s) (83000 83%) 4.2100\n",
      "678m 54s (- 129m 18s) (84000 84%) 4.2080\n",
      "686m 13s (- 121m 5s) (85000 85%) 4.1298\n",
      "693m 33s (- 112m 54s) (86000 86%) 4.1001\n",
      "701m 9s (- 104m 46s) (87000 87%) 4.1605\n",
      "708m 23s (- 96m 35s) (88000 88%) 4.0909\n",
      "715m 45s (- 88m 27s) (89000 89%) 4.0415\n",
      "723m 6s (- 80m 20s) (90000 90%) 4.0606\n",
      "730m 26s (- 72m 14s) (91000 91%) 4.0049\n",
      "737m 52s (- 64m 9s) (92000 92%) 4.0396\n",
      "745m 5s (- 56m 4s) (93000 93%) 4.0819\n",
      "752m 26s (- 48m 1s) (94000 94%) 4.0413\n",
      "760m 1s (- 40m 0s) (95000 95%) 4.0958\n",
      "767m 22s (- 31m 58s) (96000 96%) 4.0394\n",
      "774m 39s (- 23m 57s) (97000 97%) 3.9484\n",
      "782m 15s (- 15m 57s) (98000 98%) 3.9293\n",
      "789m 42s (- 7m 58s) (99000 99%) 3.9479\n",
      "797m 19s (- 0m 0s) (100000 100%) 4.0578\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data()\n",
    "\n",
    "attn_model = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttentionDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "encoder.cuda()\n",
    "decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Configuring training\n",
    "n_epochs = 100000\n",
    "plot_every = 200\n",
    "print_every = 1000\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every\n",
    "\n",
    "# Begin training\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    # Get training data for this cycle\n",
    "    training_pair = variables_from_pair(random.choice(pairs), input_lang, output_lang)\n",
    "    input_variable = training_pair[0]\n",
    "    target_variable = training_pair[1]\n",
    "\n",
    "    # Run the train step\n",
    "    loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if epoch == 0:\n",
    "        continue\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_time = time_since(start, epoch / n_epochs)\n",
    "        print('%s (%d %d%%) %.4f' % (print_time, epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our models\n",
    "torch.save(encoder.state_dict(), '../../Models/Eff_Attn_NMT/Encoder/eng_to_hin_encoder_params_{}'.format(attn_model))\n",
    "torch.save(decoder.state_dict(), '../../Models/Eff_Attn_NMT/Decoder/eng_to_hin_decoder_params_{}'.format(attn_model))\n",
    "torch.save(decoder.attention.state_dict(), '../../Models/Eff_Attn_NMT/Attention/eng_to_hin_attention_params_{}'.format(attn_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Plotting Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJztnXd4XNW1t9+l3iUXucpywTYuFNvI2IAxvZiawg2BhAABHHK5hCQkAZJAEhK+QHITUm4gMTWUQAihhd67CwYbbNx7tyxZvYw0M/v745Q506SRPerrfR49nrPPnjP7CLHOmrXX+i0xxqAoiqL0LVK6ewGKoihK8lHjriiK0gdR464oitIHUeOuKIrSB1HjriiK0gdR464oitIHSci4i0iRiDwpImtEZLWIHBNn3kwR8YvIBcldpqIoitIR0hKc90fgZWPMBSKSAeREThCRVOAO4NUkrk9RFEU5ANr13EWkEJgL3AdgjGkxxlTHmHot8G+gPKkrVBRFUTpMIp77WGAf8ICIHAl8DFxnjGlwJojISOCLwEnAzHgXEpH5wHyA3NzcoyZNmnQQS1cURel/fPzxxxXGmOL25iVi3NOAGcC1xpjFIvJH4EbgZs+cPwA3GGOCIhL3QsaYBcACgLKyMrN06dIEPl5RFEVxEJGticxLxLjvAHYYYxbbx09iGXcvZcDjtmEfDJwlIn5jzDMJrldRFEVJIu0ad2PMHhHZLiKHGmPWAqcAqyLmjHVei8iDwPNq2BVFUbqPRLNlrgUetTNlNgGXi8jVAMaYv3bW4hRFUZQDIyHjboxZjhV68RLTqBtjLjvINSmKoigHiVaoKoqi9EHUuCuKovRB1LgriqL0QXqdcV+7p47fvbqW/Q0t3b0URVGUHkuvM+6b9tXz5zc3sLe2ubuXoiiK0mPpdcY9KyMVgKbWQDevRFEUpefS64x7Trpt3FvUuCuKosQjKXruIvI1EflMRFaIyIe2wFinkJNhpeY3qnFXFEWJS7L03DcDJxhjqkRkHpY42KwkrtMlO8N6HmlYRlEUJT7tGnePnvtlYOm5A2GpKsaYDz2Hi4CS5C0xnGzbc29q8XfWRyiKovR6EgnLePXcl4nIvSKS28b8K4CXYp0QkfkislRElu7bt+8AlgvZGnNXFEVpl0SMu6PnfrcxZjrQQLTkLwAichKWcb8h1nljzAJjTJkxpqy4uF2t+Zjk2NkyjRqWURRFiUsixj2WnvuMyEkicgRwL3C+MaYyeUsMJzMtBRH13BVFUdqiXeNujNkDbBeRQ+2hKD13ESkFngIuMcasS/oqwz+L7PRUNe6KoihtkCw991uAQcBddjcmvzEmUiI4aeRkpGpYRlEUpQ2SoudujLkSuDKJ62qT7Az13BVFUdqi11WoAhqWURRFaYfeadwz0jQsoyiK0ga90rjnpKfSrJ67oihKXHqlcc/OSKWxVStUFUVR4tF7jbt67oqiKHHplcZdwzKKoihtkyzJXxGRP4nIBlv6N6qCNZlkx8lzv+7xZRz+81c686MVRVF6BcmS/J0HTLB/ZgF300mSvxCd515R72Pxpv08u3xXZ32koihKryIpkr/A+cBDxhgDLLI9/eHGmN1JXi9g5bn7/EECQUNqivDtRz7moy1V7nlnXFEUpb+SLMnfkcB2z/EOeyyMZEj+QkgZ0mnYsaWyMex8XXPrAV9bURSlL5BUyd/2SIbkL3gbdljGPdJJr2lS464oSv8mWZK/O4FRnuMSe6xTiGzYIYRb9+pGNe6KovRvkiL5CzwHfMPOmpkN1HRWvB1CYZmfPbeSl1bsjvLcq9VzVxSln5Msyd8XgbOADUAjcHknrNUl2zbub63dF3PjVMMyiqL0d5Il+WuAa5K4rjZxwjJgbabWNYdLEdQ0trCjqpGRRdnY+vKKoij9it5ZoZoRMu4b99VT57OM+5GjigB4Z90+5tzxFo9/tD3m+xVFUfo6vd64G2P9e/uXDuepbx9LTkYqS7daOe9LPbnvXn7+3Oc8oYZfUZQ+TKIx9x5Fjp0KObQgk721PgAOLykkNUUozE5nd00zAPEiMg9+uAWAr8wcFXuCoihKL6dXeu4jirJ55IpZvPWDE92xCUPyASjMTnfHNNquKEp/pVcad4A5EwaTk5HGmVOHMXVEARlp1q2EGXePdb/g7g+58d+f0awdnBRF6Qf0yrCMl79eclTYcVGO13MXjDH8Y8k2lm6tYunWKi46urSrl6goitLl9FrPPR6Rnvv7Gyr4ydMr3bFPd1THfe9jS7bxr6W60aooSu8nIc9dRLYAdUAA8BtjyiLOFwKPAKX2Nf/XGPNAcpeaGEU5Gd51saOqKez85oqGuO+96akVAPxXmW60KorSu+lIWOYkY0xFnHPXAKuMMeeKSDGwVkQeteWBu5RIz31fnS/sfFvGXVEUpa+QrLCMAfLFKgfNA/YD3dLB2mvcAbZUhhtzr3FvDQS7ZE2KoihdTaLG3QCvisjHIjI/xvn/AyYDu4AVwHXGmCjLmSw997bwbqgGAobt+8O13rd6tN93VDXx1Cc7OmUdiqIo3UmiYZk5xpidIjIEeE1E1hhj3vWcPwNYDpwMHGLPec8YU+u9iDFmAbAAoKyszBz88qPxeu4+fyAqLOPlG/cvZvv+JmaPG8SIouzOWI6iKEq3kJDnbozZaf9bDjwNHB0x5XLgKWOxAdgMTErmQhOlKDu0odoSCMY07hOH5gGwfX9T1DlFUZS+QLvGXURyRSTfeQ2cDqyMmLYNS+cdERkKHIolDdzleD33qoZWGjyNtG84cxLXnzaRm+ZNDnuPxt4VRelrJBKWGQo8bUvnpgH/MMa8HKHn/kvgQRFZgVX1f0MbmTWdSqEn5r6zOtwzv/joUgpz0tlQXhc2HmncjTEqFawoSq+mXeNujNkEHBlj3KvnvgvLo+928jPTELHUIndUhW+m5mZaapKHFOeFiY75/OHG3ecPkuXRjF+3t47qxlaOHjuwk1evKIqSHPpchWqKrQwJELS3bK+YM5YjRxWRlmrdrohwzLhB7ntaAwZjQvu7kcb+9Dvf5St/W9jJK1cURUkefc64Q3jcPS8zjR+ecSjPXnNc2JzJwwvc162BIP6gx7h3QFyspqmVu9/eSDDYKck/iqIoB0SfNO6HFOe5r0+fMjQsxOIwcWi++7rFH6TF4603t8beYPX5o43+TU99xh0vr3EbhCiKovQE+qRxv/vrM5gwxDLwcycWx5wzfkjoAXD32xupbQ411W6OYcQB9jdEqyms21sPQHqqbsAqitJz6JPGPTMtlfXlltEtGzMg5pySAdmcdfgwwFKOvOOlNe65eJrvlfXRxr281ur61OLXdEpFUXoOfdK4A/z07MmMKMyiZEBOzPMiwvdOnegeP7N8l/vaG5bxGvrdNc1RaZO1zZaETrMad0VRehAJGXcR2SIiK0RkuYgsjTPnRPv85yLyTnKX2XGuPH4cH950Sptz0lNj3/4DH2wGLK/cG4q56qGlfPnuD93jgGcTtalFOzwpitJzSIrkr4gUAXcBZxpjttkaND0epzVfJC+t3MNTn+zg+098yplTh4Wd+2xHjft6+fZQ449Ym62KoijdRbLCMhdjactsA1eDpscTz3MHuPc9y3t/+fM9cee8tz6kbKm9WRVF6UkkS/J3IjBARN6253wjeUvsPDJiGPcLjiphRmlRlA58LPbUNJOWYmXJxEufVBRF6Q4SNe5zjDEzgHnANSIyN+J8GnAUcDaW/O/NIjIxYk6X6Ll3hFhhmS9OH8nhIwtpbCOGXl7bzJ/eWM/O6iaK8zMBaFLPXVGUHkSyJH93AK8YYxrsuPy7xNajWWCMKTPGlBUXx84/70q8uekFWdb2Q0ZaChM8BU6x5v7t3U38/rV1vLe+gkF5lsSwhmUURelJJEvy91lgjoikiUgOMAtYnezFJps0T1gmN9My7umpKQwtyAqflyJhzTxeW7XXfV2QlU5GagpPfryDxpZu6SyoKIoSRSKe+1DgfRH5FFgCvOBI/npkf1cDLwOf2XPuNcZEPgB6NNm2REF6qpCVHv5rKcpJ575LZ7rH2zyt+3Iz02gJBNlR1cRNT60gGDSqD68oSreTFMlf+/i3wG+Tt7SuJdM27sYQpUVTmJ3O+CF5fO/Uidz5+jp3rKaplfzM0K/w461VzH94Ke+uq2DdbfO6bvGKoigR9NkK1Y7ieOs+f8D14h0clUlHDx7g0GFWXD7HM7a/oYXXV5fTEgiywZY/UBRF6Q7UuNv8/NypTB1RwOThBTHCMtamaXZGyJBPHha96drYEmBgrjXXmwPv8PiSbWr0FUXpEjpSodqnOXJUES9853jAEh7zUmR77jke4z5mcC4A/kC4jrud9s4eW1DMwR8IcuNTK0hNETb+v7OSunZFUZRI1HOPQWTM/fSpQwHITg89Cx1D749o0lFhK0eW2y38HPY3WuMBbeqhKEoX0O+Ne35mGtNGFYWNecMvV84Zyxm2vkymHa4ZXphFWor1OhA0LLjkKE6bMjTsGnsjPHevXHCDT1MmFUXpXPp9WGbFL86IGsvyVK7eMG8SIlasZaAde7/mpPHMGG3pxJ9zxHBOmTyUFJGw/PfyOh8t/qBbBes17jurm8I6QSmKoiSbfm/cY+EtbnK0Y8CKy7/3o5MYNdDSiN9y+9nuubys0K8yMy2FDeX1TPzpSzw+fzazxw2isiEUpqlpCnV9UhRF6QySpuduz5spIn4RuSB5S+xeHK/dwTHskeR58t3HeXq4frDBUkn2eu41jWrcFUXpXJKi5w4gIqnAHcCrB72qXki+x3MvGZDN6t21AASNwRjDE0u3u+fVc1cUpbNJ5obqtcC/gV6h5Z5scj2e+0iPDo0x8PmuWtbsqXPH1LgritLZJEXPXURGAl8E7m7rIj1N8jeZeMMyR5QUuq+DBirtVn2PXjkLgFufX8Xe2mb++dE2/vbOxq5dqKIo/YJEwzJzjDE77fZ5r4nIGmPMu57zfwBuMMYEI2PUXowxC4AFAGVlZX0q4TvTk2HjVZU0xrie+hBb+x3gD6+v57El2wD41gmHdNEqFUXpLyRk3L167iLi6Ll7jXsZ8Lht2AcDZ4mI3xjzTJLX22PxPtS8IZrm1gC1tnF3NGoACrI1UUlRlM4jKXruxpixxpgxxpgxwJPAf/cnw+5w36VlvP79E8JCNFWNrW7GTIHHuKd6HgZjbnyBoFauKoqSRBJxH4cCT9ueaRrwD0fPHaKlf/sKb1x/Qod12U+ZbFWp7qkJVac+9+ku93VWeirfOXk8f3pzA9URm6rN/gA5GerNK4qSHJKm5+4Zv+zgl9X9HOLJVe8oXmngSL5/+qG8sGI3/1i8LWy8sUWNu6IoyUOtSSeQ246RdnRpvDTFaMh919sbyM1I49Md1ZwwsZjzp41M2hoVRenbqHHvBFJS4mcMQUgh0ktTjAbbv3l5rfv6qU92qnFXFCVh+r0qZGdzx5cPjxrb3xBt3HdWN7GjqjFqXFEU5UBQz72TOH7CYI4sKeLCmaUcUVIU1rpvRFEW2/c3AXDFnLHc9/5mLn/gIyAkRhYre6amqTUsnbI7uevtDQQChmtPmdDdS1EUJQbquXcSD18xix+ccSgAk4cXuJ2bAB67ajZH2hrywzwFTwDltg58ZDYNwLbKnuPZ/+bltfzutXXdvQxFUeKgnns3UDIgh6e/fSwfbdnPgNwMbntxtXvu6P/3BtefNjEshdKhot6SDf5wQwX7G1s454gRXbZmRVF6F0mR/BWRr4nIZ/acD0UkKnVSCSclRZg1blBYuMbhmeU7WR+jkfY+27hffO9i/ucfy6LOV9b7osac8bfW9ks9N0Xpt3QkLHOSMWaaMaYsxrnNwAnGmMOBX2Lrxyjt423p57CpoiHm3Io4xhvglc/3cNSvXo9pxL/96Cdc/sBH1DarGqWi9BeSEnM3xnxojKmyDxcBJcm4bn8gJ4ZxN3GUCPbVhRt376br/725AYA3V0cb9x37G+1/mw50mYqi9DKSIvkbwRXASwe3rP5DVlr8alYvKQIV9eEplHd75IJ3VVuGe/n2anfMMf6Fdu/X7TFSLSvqfTFTMxVF6d0katznGGNmAPOAa0RkbqxJInISlnG/Ic75PqvnfqC0V/DkMGZQLpsr6nl44RZ37LevrKW6sYXm1oCrGb/FDum0BoKM+/GL/OmN9RTZ6ZM7qqI997Jfvc5Rv3rt4G5CUZQeR0LG3Sv5CziSv2GIyBHAvcD5xpjKONdZYIwpM8aUFRcXH/iq+yinTh4S99xFR5eycmctNz/7edh4dWOrK1Q2vDCLxtYAxhheWrkHgPs/2Eyq/QDZvj92KmW8MJCiKL2XpEj+ikgp8BRwiTFGk58PgNGDclhwSRnxHPkrjx8bc7yqscUNyYwfkkcgaPD5g3yy1doCmVE6gDp7I3V3Tbjn3tjiT9LqFUXpaSTiuQ8F3heRT4ElwAuO5K8j+wvcAgwC7oqXLqnE55ObT+Pl6+aSkiJhmu9gVbqC1QzkqhgG/sdPr2SjHYpxlCwbWwJu2mR9s5+6ZsuI760N35CNDNM0twa49P4lrNpV2+Z6jcfVD6gOvaL0SJIi+WuMuRK4MrlL6z8MzM1wXxdkpRMMGmptg3zfpTNdUbGb5k3msx01LN68352/enctv3/VEhgrHZgDQIPPT4WdWVPb3Opey6l+dYjUslm5s4Z31u2jtrmVp//7uLjr9XsMeos/GDOdU1GU7kXlB3oY3zhmNPPnjnOPM9JSXD2ZlBRxX18ye7Q7p6qxlRSBIQVWj9bGloCbE1/X7HfDMuV1Pjbuq6fZfljsqbHmpKdasSDHZre3xettYtLi71hDE0VRugY17j2MK48fx4UzS+Oez7Abcc8YXRQ2XpCd7vZubWjxu2mTFfU+fP4gxfmZ+IOGU373Dj9+egUA1U3WHKdJiGOoP9lWzVtry6ms98UMu3gNui8QLVWsKEr3o9oyPZC2Ojk5xr01EG50C7PT3SYhNY2t1DS1IgI+2xCPL85zi6AWbqykNRB0s2ycdq71vlAFq6NSecWcsdx8zpSwz/Iad/XcFaVnop57D8TRm7ns2DFR5zLtoiefP4inxzaF2elutevlD1qGeZxHiXLy8AL3dVZ6Kpc9sISHFm4FQgbaic17eWzJNp5dvjPMg2/xhGUiHzKKovQM1Lj3QESE9bfN45YIjxngquPHMm5wLmdOHca6X83ji9Ot7kyFnrCMw9yJoVqCKSNCxn1zRQMfbAiVIvj8Qd5aUx4zS6axJcB1jy/n/vc3u2PquStKz0fDMj2U9NTYz91xxXm8+YMT3eMJQ630xxSRMJ2a0oE5zB43iAc+2ALApGH5cT8rEDSutx8Pb2ZNi26oKkqPRz33Xs54O7d9X50vzLi//N3jGTUgxz0eXpgV9d6OEBaK8XtDNLqhqig9kWTpuYuI/ElENti67jOSv1QlFuOHWMa9vM7nZr2AlQEzZnDIuBflZIS9zymOShSfNxTjMeg+9dwVpUeSLD33ecAE+2c+cHcyFqe0T+nAHIrzM7n5nMmkpggDczP4od3ez2vsUyN0DSYOjR+miUVY+qPG3BWlx5OssMz5wEPGYhFQJCLDk3RtpQ3SUlP46Cencv40a2P1k5tP45qTxsec+7v/ChUalw7MYWhBJtNLi2LOjcRrxL0ZMvGyZQJBw/3vb3YLphRF6VqSpec+EtjuOd5hj4Whkr9dz4vfOZ5/f/sYAL58VInbmLswO53FPz6VS48Zk9B14m2ixvPcX1yxm1ufX8Wdr1s6cs2tgSj5A0VROo9Es2XmGGN2isgQ4DURWWOMebejH2aMWYDdgq+srEwTpLsAbwokQCBoGeP8LOs/vVMU5XD24cPJyUjlXx/vCBtv9IU8cK9Bj+eZO6GbjeUNPLJoK698vof31lew5fazD/BOFEXpCMnSc98JjPIcl9hjSg/Db4dRHPXJzAjjPmpgDtNLB0S9r6YpVL3q7cW6ZPN+jDE88MFm1zMvr2vmX0utL3Kvr97LT59ZyXvrKwBLUXLhxkqufWxZmLokWJWzx/z6DRp8KkWsKAdLUvTcgeeAb9hZM7OBGmPM7qSvVjloHEXHvMzYnvvA3PSY8gdeg75ubx05Gal8YdoIXlq5m437GvjFf1bx7Uc/AeDiexaHKVd6aW4NcvmDS/jPp7uoizDid7y8ht01zazZ07bksKIo7ZMsPfcXgU3ABuAe4L87ZbXKQeO3Y+eOUc+IKJbKzkgLy7JxaPKEX9buqWPC0HzOmzaC2mY/b6+1mnLvtpuGbCivj/v5Ta0BVwNnf0RPWEedUiUNFOXgSZaeuwGuSe7SlM7gd185kj+8vt7Vfo9MkcxMS6FkQHbU+xpbLOPe1BJg1e5aTp8ylDnji8nPTOOFFdaXNJ8/2G5qZFNrgNzMNCobWqhs8DHGo3+TlpLifkaitAaCtAaCMR9IitKf0QrVfsZRowfy8BWzXHmD+ojQyOC8DA6NkQPf4g9a6Y0fbKa6sZUvTB9JRloK44fm8bmtSePzB8Ni87E47vY3qbKbeVfWO//6uOS+xey1Y/btXcPLNx/8iCm3vJLwfEXpL6i7089xipn+dNF0WvxBTjp0CCLCzDED2FvrY5unqfbPn/uchxdtZWRRNsceYlW4jhmUy7Jt1QD4/IGoh0UsnFh7pW3k31hd7m64AuysbmJbZSOlg3Jivt+L932KooRQ497PGVGUHTM98Z/zj0EExt70ojv28CJLInhAbqjP62iPAW4NGLfrUyJU1vt4aOEWVu+uCxv/7Str+e0razVtUlEOAjXuSkxSUuI32xvg0akZMyg37FxdDE34ycMLWL07lAGTIlZLv1W7a3lxxZ4DXuNba8rd162BYFwlTUXpj+j/DUqbbLhtHn++aHrYmNe4Dy0IV5t802NwAX55/lSmRhRSOX0/Po+hH+8lVos/L16ZYpU5UJRw1LgrbZKWmuLmxDt48+AH54WrTd7naeoBcMkxYyjKTicWWytD8fxYksQ+f3yD/d76cPmK5tZQls5/Pt3FI3YISVH6KwkbdxFJFZFlIvJ8jHOlIvKWff4zETkructUupPsjPCiJq9HPSgv03198qQhMd9fGMe4eznzsGFRY16DDbByZw3NrQEq6n1cct+SiLmhB8G1jy3jp8+s5IEPwh80itKf6Ijnfh2wOs65nwJPGGOmA18F7jrYhSk9B6enq4NHQyzMKx8dJ7vF0bFpi9OnRBv3XXZRFMDGffWc8+f3+d2ra1m7py5qbiwv/xf/WUV1Y0vUuKL0BxJt1lECnA3cG2eKAZzAaiGw6+CXpvQUIj33gZ5sGe/Gq1MYFUlWerScgcPEoXks/empHBqjDeA5f36f9XstQ/7BBivlcWd1E5/tqImaG+nlO+yoaoo5rih9nUQ99z8APwLilR/+HPi6iOzAkiK49uCXpvQUvJ77z8+dwvdOmxhzXjzjHqlf42VYYTaD8zLdbwCR3r8jZbDE1qqpbfJzx8troq7j9dzTPA8cb+/XyPn7G0JefUW9j8YWFSxT+g6JCIedA5QbYz5uY9pFwIPGmBLgLOBhEYm6tuq59068nvtlx42NW+o/Z8JgTjy0mC9OD5fyz0yL77k7G7IpKcKW28/me6eGPzgCtnKkU826bFtVzOtc+LdFXPWQ1QGyOD/T3QSO57lf8eBSZvzyNfe47Fev86W7Poy7TkXpbSTiuR8HnCciW4DHgZNF5JGIOVcATwAYYxYCWUBUk05jzAJjTJkxpqy4uPigFq50HTkZ8Y0zwB+/Oo2fnTuFzLRUHrz8aO68cFrY+bIxloTwzedMcccckbD8iEyceYcPY8rwUOqkI1HsqFI2xNGd8QcNr63aC1g6OF+aMZL8zDS274/tub+/ISRB7LAmRixfUXor7Rp3Y8xNxpgSY8wYrM3SN40xX4+Ytg04BUBEJmMZd3XN+whZtud9fZxwzPnTRnL5cWPDxp655jhe+95cwMqF33L72VwxJzTHKTiKjMdnpqXyqy8e5h7vq/NhjAmTHG6PptYA2RmpjCjKZndNdPcnr7jZntpmKup9CV87kmse/YR739t0wO9XlM7igCtUReRWYKkx5jngeuAeEfke1ubqZSayE4PSa3FCJh1h2qi2e7M6fx2xNluzPGGc215cTUF2GjWNiRn3QNDQ4g+SnZ7K0MIsV4zMy8zbXndfH/PrNxO6bjxeWLGbF1bs5srjxx3UdRQl2XTIuBtj3gbetl/f4hlfhRW+UZSEmDqigKVbqzhsZGHUuaz08C+UL63cE9XYQyT0gPDi5LvnZKQyrCCTtTEaf3REdVJReitaoap0C5ccM5qXrjue06YMjToX6c1v29+IMTDCU8Ua2R7QwdGdz05PZVhBFvvqfG6DkmTTnjyConQnatyVbqE4P5PJwwtinos03Jv2NVjvKfAa99ibvE6jjyw7LBM0sNKjYfPPj7a1ua7a5lYWbqxs/waABk2dVHowatyVbmFIfmbcc/GKnrzviQzdOFTZFak5GWmMKLI6Sl16vyVVUFHv44Z/r2hzXdc8+gkX3bMoagO3xR/k/Qjt+PoYCpiK0lNQ4650C8V50UJhDvGMu7eYKrIwanqptYHrFCZlZ6QwZ/xg0lPFjbF/ur263XU5zT+c2L2TF3DHy2v4+n2Lw67RkEBjEkXpLtS4K91CQXb8vXxvX1evIuWM0lAGjtNvFeCio0dx0cxSADetMSs9lfTUFL59wiGIQE1jK99+5JOE1+ez5QyOvf1NvvPYMtbZMgj7PVo1kZu8itKTUOOudAsi8ZuBAFx27BgevHym60G//v25TPToz3jfPXl4gVtF63juThXtgNwMjLFSFls8G6vtFWY1tQbYUtHA7ppmnvs0JJXk/Vz13JWejBp3pUsZmJvR/iTg5+dN5cRDh+C3M1KGFmSFh2tsK/uHC6dxyezRrrF2ipYcJUqnsciPnw6Ptd/mKZSKRYPPz3ceX+Zea1+d9Y3Am36pMXelJ5NwnruIpAJLgZ3GmHNinP8KloCYAT41xlycrEUqfYf3bzipQymEj1wxi6c+2UF+VnpYzN3xoKeMKEBEXM99U4WVWTPMzqwpyomtJT8kP37MH6y6nOdIAAAfN0lEQVRceEd9sq7Z70oTNHl049trBh4MGppaA+RmajdLpevpyF+do+celb8mIhOAm4DjjDFVIhK7a4PS74knOhaPORMGM2eCJVPk9dynjChk474G12N3rru5op6CrDTXoEZ+U8jPSqOu2R/VXSqSP72xPub4xvJ6Pt5aha810K5x/9q9i1m4qbLTG30bY9hZ3UTJgNiqnEr/JFl67lcBfzHGVAEYY8rjzFOUA8brud/x5cP5x1WzXIOWaxv57fubGOYpdvL2e4VQO7/2vjt8si12Zs3vXlvHl+/+kIvvXcwzy+O3LdhX52PhpsTy5Q+Wxz/azpw73kooG0jpPyRLz30iMFFEPhCRRSJyZqxJKvmrHAze3PacjDSOPSQkPDqiKBtnj3ZYYbY7Hum5//XrRzF/7jgOH1lIQQIdotrqIuU1pk4VbGsgyJRbXub/3lwfdS4YNHSG5NLSLZYMspPRoyiQPD33NGACcCKWtvs9IhKlHKWSv8rB0FZHp9zMNEbbzUKGeypZczPTuPtrM9zjsYNz+fFZk0lNEUrjtAX0kkj/V8DNxFm8aT+NLQH+vjDUoNtnq1CO+/GLXP1I6H+jl1fu4c7X1rnHza2BsP2IO19bx4+e/LTdz3aak6gcguIlWXruO4DnjDGtxpjNwDosY68oSSOenoyDY/xPnxquV+MVJ/OmYF45p30lx/aMu7Nh6+TFv73WikgO8nxjaG4NuB77K5/vdcevfuRj/uiJ7Z925zthTb3/+MZ6nli6o901ptna+K1q3BUPydJzfwbLa0dEBmOFaVTkWkkq7eXG//pLh/PdUydw8qTw/fx42SpfmD6S9bfNC2sO8n8XTw+b055xnz12EAB/X7iFXdVNlNspk94CJ58/2O7ma4s/yPb9TSzevJ9tlbEbjMTD8dxvfmYlK3dG95dV+icHnOcuIreKyHn24StApYisAt4CfmiM6ZrdJEWxmV46gO+eOjHqIZCbGT+ck56awr+uPoYXvjOHZ685jnOOGBF2Pl4qpcMJh1rhxT+8vp7bXlhNna1J420I4vMHqahvifl+sHLqnfe9tmovc3/7Vty5Dy/cwg/+9WlY/9dUT7XujU991uZ6D5Z31+3j8geWdMregZJckqXnboDv2z+K0qNoq4crWJ791BHRuvIAuW2kbv7yC4dRkBUy/qt318aUJGhuDbRZzTr1Z6/wxLeOaXONYDX1vvnZzwGYPW4QFxxVAoRaFkJsjftkcuXfl9ISCNLcGgzrrav0PLRCVelVFOWkM++wYV32eWmpsUNBaSnCJbNHhwmYbapocCtZvXznsWXsrA416l66ZX+Usf9oy/6w41gPg8WbQnM27at3X6ekdJ1xN3YSqfebidIz0dI5pVex/JbTu/TzUuwQz7lHjmDN7lpaA0FuPf8wRtuZNlUN8cMtDuvL63l8yXb3+IK/LuSMiE3fyDTG7VWhuLsxBhFx5wzOy3A17gGCXRgicT7KFwgAiWUSKd2DGnelXzBucC6Thue3PxF46brjmffH9wAotdMrp44o4M8XTY+ae8bUYSzcVMniTZXsitGM26GqMfwh4M2aAVypAwfvpmpLIEhmWqq7KXtESREbPZ57q79jxn1LRQMBYzikOK9D74NQ8ZeTHaT0XDQso/QL3vzBidz1taMSmuvtEDV3YjH/uvoY5sdpgF2Yk86dF07jinYaZNe1IzK2uaIh7LjS843AyZNv8PnJSk+hdGAOezyNv1sCIb2btsx8eV0zxhhO/N+3OeV377S5nng4G6kvr9yjm6o9HDXuitIG6akpzBwzMCyuHYtvHjeGb50QbuDzPSmY+xMI33hxesFCqHFIvS9AXmYa+Vlp1Pv8BIPR8e94/WJX767l6Nve4PGPQuGhv76zMaG1fLy1it/bxVaOOb/txdU8vWxnwvejdD1q3BWlDTJSE/tfRESicuIHe9oCetUkHW49f2rc6zV68+RbQ557bmYaBVnpGBPq4doaCHnQTtcpsLzs5dureXjRVt5cYxVXvbM2JPtx+0tr2FsbP5Tk8OW7P3SF1LzO+u42wlBK95M0yV97zpeBJ4GZxpilyVmionQfke382iIrIuWyZEB2VLjFy6FD4+8BNHoeBgs3VRIIGsu4Z6S5ejd1zX7ys9LDPHevcV/w7iZ+/dKasOvuqw/P5vlkaxXzDh/exl2FiAzDtFNTpnQzHfHcHcnfmIhIvj1n8cEuSlF6CulxUiFjcUFZCWdOHcaXZowEYNTAtrVrDhmSx8wxA2JWwXo99x89+Rkn/u/b1Pv8dljGmu/E8X0RBVONtkd/5+vriGRHVXj168JNlUz8yUs8u7z9EIs/Qt5gw956bn5mpRsecli/t45z/vxeVJNxpWtJluQvwC+BOwD9rqb0GdI74LkXZKXz10uOcgubRhZlh50/edIQfnjGoSy7+TT+OX82g/My+dfVx7oPAy/1vugwTkOLn9zMVNdzd4xnSyDI8MIsvnfqRADKa30YY2iOkdGytzbcc1+4sZKWQJBb7OKotvAHwo34U8t28vCirWzbH/7A+N2r61i5s5b31lW0e02l80iK5K+IzABGGWNeSNbCFKUnkGjM3YvjUQ/OC5cbLs7L5JqTxjMgN4NZ4wa547Eah3jDKw4NPqurk2PcX1+9l4+27KfVH2TUgByOGj0AgL21zVQ1tu01/+PKWZx35AjWl9fH/bxIWoOxN2v9EeNO3n07e9BKJ3PQkr8ikgL8Hrg+gWupnrvSq0g/AOPueNReaQKIrw0fS9ispik6u2ZnVVNYWOZv72ziv/66kJZAkPQ0YWiBtYG7t87HzqqmqPd713HMIYOYOLRjee6RnrtDZJqnE6VpT+ito3y4sYKK+ugKYCU2yZD8zQcOA96258wGnhORssgLqZ670ttIPQD30wnHjByQzZElIc2avA4Y9+oYnndLIGhny4TPb/EHyUhNYYitY19e2xwVW/cydnAuItJuEdPGffVcZzcJh/hplpHG3dl4jfzdtQaC/O2djfj80SGn9ggGDRffs5iL71nU4ff2Vw5a8tcYU2OMGWyMGWPPWQScp9kySn/lxnmT+Ps3j+aIkiKe/Z85TBpmZcXkZ8Uu18+LoVpZHSdMkpWeEnUdnz9ARloKBVlpZKensre2mXV762O+HyzjDlAU0YKwqSXc6H757g951tNKsCVR427/G/lYfHzJNn790hrufW8zHaXe3iRu676UcJIl+asoik1WeionTAx9M51hx8LH2UY1kozUaONeE+G5/+oLh3HypCGcdOiQsHaDYG2SZqSlIiIML8pi+/4mXv58D9NLo5qhATDvMCv1MVIKuc5nfea6vXVsq2yM+vYQPywTmrd6d62bVdMa8TBwNokPJIum3n6AdCQ1tb+TFMnfiDknHuyiFKUvcdsXDuNn506JKz0ciMgfz89Ki/KGZ44ZyNdnj3aPM9NS3BTImqZWN2Vz4pB8Xv58DwA/PXsyyzyNvicOzePJbx/r7gXkRMgZN/gCkA+n3/luzHW25bmv21vHJfctDsvGaQkEqWtuJSMthcy01NAG6wGoFji/j8wD2ANJJh9uqOCwksKo/ZSeiD4GFaWTEZE2NeUj88RjGY6C7HBDfNz4wWHHTgvCCfYm6YzSIi47dkzYnKLsjLBrR3rubWnOA3G7SdU1t/KjJz+LSrP0+YMc/vNXufiexQSDxi168t7txn31XHzPonY7VdXb3yoiPfd6n59Vu2rbfG+yqG5s4eJ7F3PNo590yecdLGrcFSUGV59wSNhmaGdyyuQhHDc+lL1S7JEtcCjKDo+P3/mVaVx78nj32Hl4OL1bT540hLQIL3dQRGpmpOde7/O3aWTjGf/aZn9MZ9ypnP14axXjfvwiC961Om96K11/9+paPtxY6cojxKM2Tljmqr8v5aw/vRf1gGyPmqbWDr/HuZ/Vu+vamdkzUOOuKDG4cd4knv2fOV3yWflZ6Tx65WxXXtjrcd/1tRk8Pn92VNejwpx0rj/9UPd4oi1lcOHMUn54xqFcaatUetMvZ3ty6wFyI675y+dXcdjPXom7znjKlnXN/pgKkb6Ihh5Oq0Hv1EG51oOssp0Ux3gx98WbrW6e8UJGsahubOHIX7was4K3LXpb/3E17orSQ3CMXk5GqpuzPiAnI8oox+LosdambXZGKtecNJ6sdMtwv3n9ifz+K0dy9NiBfDGiEjbSs/+8nfBGrPRMsLzgQAzLVxsn48c7c6D9TSMyf/2ddftYsye0HufBEllU5lwrcvO2LRw55ec/253wezr6GT0BNe6K0gMZM8jKrGnPI735nCmMLMqOm7NenJ/Jl2aU8MS3jjmgTUCv7s2Pn14Rc05FvS9mR6rqxtgyx7Fk4CPbE156/xLO/MN77rETc19fXs8HG6JlDVrjZPIkk8hvIj0dNe6K0kNwipwy0lL4st382omhx+OKOWP54MaTk1YNevlxY9zXWekpMUXNTpsy1N3ABUvuYG+M3rH743j6BsP2/Y3Mf2gpf7SlhHdVNxMIGt5aWx4W4nHi3N6Q0NfujdYm7IhX7SbtdLDZiPMZFfU+Pt1e3c7s7keNu6L0EH5x3lR+cPpE5k4o5itlo1jyk1M4bGTXbOo63HLOFFeALC8zPSqnHuA7J0/gnm+UMb20iCNLCtldYxnmkycNCVPRjOe5t/iDHP+bt3h1VajVYF1zK/e9v4nLH/iIN1aHNldve2EVPn8gKt7/wAebWb692jXULf4g/1q6nfK6Zt5eW95mI5IDjZ17pZXP/8sHB3aRLiQpeu4i8n3gSsAP7AO+aYzZmsyFKkpfpygng/85eYJ7PCQ/q8vXICIMK7Ti/QXZaRw2sjCqKjQnM5XDS4qZO7GYe9/bxKd2/9eLjy6lICuNZ+yq1njdp15auSdqrKk1wOYKSzJhe1UjIlb45u8Lt3LIkLyoTJ1f/GcVEBIn21XdxA+f/IwjRxW5XvXVJxwS8/MPNHbel2Pubem5LwPKjDFHYDXr+M3BLkxRlO7B0brJz0rnjKnDos97Uii9aZvDCrPCJA3ibcDGMvqNLQGcvdL9DS0YA98/bSKZaSks31Yd1jM2Fk4Dcq9gWl2cSlin0rajDnxLX4y5t6fnbox5yxjjKBUtAkqSszxFUTqTIbZxHpibwSmThgChnPmCrDROnzKUn549Oew93uKnwXnhxv0k+xrQsb6xza0BKu1UyXV7rTzyrPQUinLSeWrZTt5bH72JOig3wzXQ++qdzwqZ7HgtBB3p4o729+5IumVPINGwjKPnHr8vWIgrgJdinRCR+cB8gNLS0gQ/WlGUzuKdH56EPxgMEyNzQiAF2emICGdMHcavXgh9afcWP00ZXuC+HpiTwdwJg/nmcWO5/4PNMfvGxqOxJcAuuyfrejsMlJWeyoCcjKjKV4fBeZnstz32CntD1xtP313TzPgh0SYrnkZOe/Q5z709PfeIuV8HyoDfxjqvkr+K0rPIzkiNUpk89pBB5Gak8q25ViFUZOGQV8p3QG4G3zllAmdMHUpKiiAi3HLuFFcJMx6RWvJNrQF2V1shlS2VVt/ZzLSUmI1MHHIzU13vu7LBMu7efPvd1XE89wOOufeuKqZEPHdHz/0sIAsoEJFHvLK/ACJyKvAT4ARjjCrqK0ovZUhBFp/feqZ7nNaOpv33T5sYNZbZjnqjVZka2qg1BsojvO+s9NQoUTUvn3hE0SrqWuz3GldULV6cvj3jvnBjJSLRFb0tgY7r0HcnB63nDiAi04G/Yem4ty0SoShKryKykjURGm1t+DGDYjcJ/+rRoxg1MDtqvGRAaCwzLTXhUIhT4RoIGjc2HmtD9cTfvsXtL60BrHz7WFx0zyK+uiC6KUirv3d57snSc/8tkAf8S0SWi8hzSVmdoijdjjd3PVEcY/uni6bHPF86MIeXrpsbNT7ZE8PPSk9JOITifF5jS8AN1Xhz4x2RsC2VjazZ0zHhL6fYKRkbquV1zR0unjpQkqLnbow5NamrUhSlx5CWEvIB7/rajITec++lM2ls8XNESeyGIQNyMqKEy8Ay7q/ZxU1Z6akJ97CtqI/OzHn8o+00tAT4+qxSLlywiPsvi+r82SY+fwCfP8gJv3mLwXmZfPXog0sCeXnlbq5+5BP+/s2jw5q5dBZaoaooSpt4PfezDh+e0HuOGj2A4ydYBmzRTafw6JWzws4PyM2wNl89FbEAU4aHNmKz0lPjev6RxJMq/s+nu1i92xIg+8nTK8POxXKgvf1d99Q0s3xbNVWNrawvr6cmjhBaovzvq5YK5aZ9XdMqUI27oihtcrC6NcMKs5g6oiBszGny/c05YykbM8AdnzoiJLeQlZ7CIcV5/OQsK8/+sJHh14jkes/GrveB1GDH/yMfALGMu7fwamdVE5/tqPacC/928OsXV7O8AxozzXZq6ME+JBJFjbuiKJ1OZGMQ7wPDkScGGFkU2lDNsoupnKllowfy4OUz437GGYeFqmnnTgiFPTaWW55yPD16h6eX7eC//rrQPV67t47l22vc46qIitu/vbuJL/zlA3ZWN5EITgPySAXMzkKNu6IonY43V/5n504JO5fjib2npEQb/RTbuhtjovTcvRRmp/Pc/xzHm9efEJYHs3xHYt71Lc98zrb9je7xm2vKWby50lXmjCeEduO/P2v32sYY9+Gixl1RlB7F+dNGJOU6lx83Nuy4wJYVjuz56uTKO/Y+aNrOWCnMTueIkiLGFeeFpVBu2teQ0LpmjA6Fh44ZN4j31ldQ1+zn9KlDgZB+TSSJXN/nD7prL+8i496hbBlFUfonW24/u9OuPbIom1e/N5fxdsORMYNy2FLZ6LYWHGB7zgNzM1wvPhbe8I5jSKeXFrFsW2Kee4s/yMDcDC6cOYpDh+azcJPVwu+0KUN5bMl2qhpix8p31TTh8wfabIL+vkcbp6s892RJ/mYCDwFHAZXAhcaYLUlcp6IofYB4RU1OD1iAJ751DIs273eN9blHjKC5NcAXp5eQmiJ879SJrNhZzeur49dLOvnx00bFN+47q5t4d90+5k4sZsWOGpZvr2b2uIHccOYktnvCM05v23ieuzGwo6opbjesfXU+rnxoKQCHFOeyqaKBmqbWmI1QkkmyJH+vAKqMMeOBO4E7DnZhiqL0LT77+em8/N3owqVIhhRkcd6RoRBQSopw4cxSMtJSSE0Rrjt1gitLHI+vzhwFWF53W3zj/iU0+Pyc+3/v09QacHV2SgZkk5ORymXHjnE3g52q21hsq2yMe87bFvDkSUMwBpZtq2pzXckgKZK/wPnA3+3XTwKnSLL6fimK0icoyEoPC50cDF7jH4sLZ5ay5fazOcoTR4/HFzxdlfLtFE0RYdWtZ/Kzc6e0+yAB2FoZHnc3xrhVse+u3+eOz51YTGqKsHRLDzHuhCR/4+1mjAS2Axhj/EAN0H7LdkVRlAPglMlD+ef82e3Oy0xLZcElR/GbC46IeT47PZX15aGiIqePrYOIhFXSTi8tisr2AfjL2xvZ6ClOuuqhjzny1lcxxoTF24cWZHHr+VNjNkFJNkmV/E3gWvNFZKmILN23b1/7b1AURYlDot8CTp86LEx33ssvzpvKgJxQ7DsrxqZoWmqKm7lTmJ0eU0htX52PK/++1D1+ffVe6pr9vLpqb1h2THZ6Kl+bNZrDSzq/N26yJH93AqOAHSKSBhRibayGYYxZACwAKCsr610Sa4qi9CiOKCnkh2ccSl5mWrvhl3iblyOKsvnyjBLufX8zYKUsxiI3Mw2fv4XC7HTS40ggV9T52FfnC2s9+K2HP2bUwGweu2o2H2+tYtTA2BvKnUFSJH+B54BL7dcX2HPUeCuK0mmICNecNJ5Ljx3DYSPb9oRHDczhDxdO4/lr54SND8zN4LunTeQYW7u9OU73KOfh4PXcRw3M5vrTJnL5cWMAqPP5mXnb6yzZvN+VVwCYNXYQJQNyOH/ayAO6zwMlWZK/9wGDRGQD8H3gxmQsTlEUJVl8YfrIqIdAUU46eZlp3DBvEgCzxg6M+d6hBZY3Xpid7urWlBTlcO0pE7jlnCnM9ag8fuVvC8O+AbTXuKSzSJbkbzPwX8lcmKIoSmcyKDfDDaFMG1XEp7ecTmFO7PCNkyJpGXfLWGemW/+KCMWeRuFAhHFPToZQR9EKVUVR+iUf33xa2HE8ww64GTP5WWmuHIJX56athibOQ6CrUW0ZRVH6FYPzMjr8nmy7kMnSiLG2E71iaI6WzRdi6O/0irCMoihKb+e9H53cZuPtWEyx9eiL8zJdfXhvuMUJwwwviu4L211hGfXcFUXpV2RnpJKXQNWpl6/PKuUfV87izMOGud2avJ771SccwvDCrJjKmeq5K4qi9FBEhGPHDwZC+vNOBg3A4SWFLLzplDCpYYfuirmrcVcURekA5x05knpfgAvLRkWd83rz+Vlp1DX7NSyjKIrSG0hNES6ZPTrMkHu5cd4kHrh8Jk7+TI8Ny4hIFvAukGnPf9IY87OIOaVYqpBFQCpwozHmxeQvV1EUpWdz9QmHAKE+sd1l3BP5VB9wsjHmSGAacKaIRMqx/RR4whgzHUui4K7kLlNRFKV3kiyZ447Srudua8Q4Wpbp9k9kHpEBHNm1QmBXshaoKIrSm+nJnjsikioiy4Fy4DVjzOKIKT8Hvi4iO4AXgWvjXEclfxVF6Rc47Yoyu8lzT8i4G2MCxphpQAlwtIgcFjHlIuBBY0wJcBbwsIhEXdsYs8AYU2aMKSsuLo48rSiK0ufo0Z67gzGmGngLODPi1BXAE/achVi674OTsUBFUZTeTI817iJSLCJF9uts4DRgTcS0bcAp9pzJWMZd4y6KovRb3FTInrqhCgwH/i4iqVgPgyeMMc+LyK3AUmPMc8D1wD0i8j2szdXLtFmHoihKuHpkV5JItsxnwPQY414991VY7fgURVEUrGbbVY2txOnK1+mo/ICiKEon8NA3Z/Hiit0MzO24xHAyUOOuKIrSCYwdnMs1J43vts9XbRlFUZQ+iBp3RVGUPogad0VRlD6IGndFUZQ+SCJFTFkiskREPhWRz0XkF3HmfUVEVtlz/pH8pSqKoiiJkki2jCP5Wy8i6cD7IvKSMWaRM0FEJgA3AccZY6pEZEgnrVdRFEVJgGRJ/l4F/MUYU2W/pzyZi1QURVE6RrIkfycCE0XkAxFZJCKRwmKKoihKF5JQEZMxJgBMswXEnhaRw4wxKyOuMwE4EUsW+F0ROdxWkXQRkfnAfPuwXkTWHuC6BwMVB/je3orec/9A77l/cDD3PDqRSR2qUDXGVIuII/nrNe47gMXGmFZgs4iswzL2H0W8fwGwoCOfGQsRWWqMKTvY6/Qm9J77B3rP/YOuuOdkSf4+g+W1IyKDscI0m5K6UkVRFCVhkiX5+wpwuoisAgLAD40xlZ22akVRFKVNkiX5a4Dv2z9dwUGHdnohes/9A73n/kGn37NoTw1FUZS+h8oPKIqi9EHUuCuKovRBep1xF5EzRWStiGwQkRu7ez3JQkTuF5FyEVnpGRsoIq+JyHr73wH2uIjIn+zfwWciMqP7Vn7giMgoEXnLo0l0nT3eZ+87nlaTiIwVkcX2vf1TRDLs8Uz7eIN9fkx3rv9AsQshl4nI8/Zxn75fABHZIiIrRGS5iCy1x7rsb7tXGXc7Y+cvwDxgCnCRiEzp3lUljQex6ge83Ai8YYyZALxhH4N1/xPsn/nA3V20xmTjB643xkwBZgPX2P89+/J9O1pNRwLTgDNFZDZwB3CnMWY8UAVcYc+/Aqiyx++05/VGrgNWe477+v06nGSMmebJae+6v21jTK/5AY4BXvEc3wTc1N3rSuL9jQFWeo7XAsPt18OBtfbrvwEXxZrXm3+AZ7HqKPrFfQM5wCfALKxqxTR73P07x0ozPsZ+nWbPk+5eewfvs8Q2ZCcDzwPSl+/Xc99bgMERY132t92rPHdgJLDdc7zDHuurDDXG7LZf7wGG2q/73O/B/vo9HVhMH7/vSK0mYCNQbYzx21O89+Xes32+BhjUtSs+aP4A/AgI2seD6Nv362CAV0XkY1t6Bbrwb1sbZPcSjDFGRPpk3qqI5AH/Br5rjKkVEfdcX7xvE6HVBEzq5iV1GiJyDlBujPlYRE7s7vV0MXOMMTttCfTXRCSssr+z/7Z7m+e+ExjlOS6xx/oqe0VkOID9ryOl3Gd+D2L1CPg38Kgx5il7uM/fN1haTcBbWGGJIhFxnC3vfbn3bJ8vBHpT9fdxwHkisgV4HCs080f67v26GGN22v+WYz3Ej6YL/7Z7m3H/CJhg77RnAF8FnuvmNXUmzwGX2q8vxYpJO+PfsHfYZwM1nq96vQaxXPT7gNXGmN97TvXZ+5bYWk2rsYz8Bfa0yHt2fhcXAG8aOyjbGzDG3GSMKTHGjMH6//VNY8zX6KP36yAiuSKS77wGTscSW+y6v+3u3nQ4gE2Ks4B1WHHKn3T3epJ4X48Bu4FWrHjbFVixxjeA9cDrwEB7rmBlDW0EVgBl3b3+A7znOVhxyc+A5fbPWX35voEjgGX2Pa8EbrHHxwFLgA3Av4BMezzLPt5gnx/X3fdwEPd+IvB8f7hf+/4+tX8+d2xVV/5tq/yAoihKH6S3hWUURVGUBFDjriiK0gdR464oitIHUeOuKIrSB1HjriiK0gdR464oitIHUeOuKIrSB/n/ORmf971c0FgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=20):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "\n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[Language.sos_token]]))  # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_input = decoder_input.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context,\n",
    "                                                                                     decoder_hidden, encoder_outputs)\n",
    "        decoder_attentions[di, :decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        ni = ni.item()\n",
    "        if ni == Language.eos_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "\n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 Evaluating randomly chosen pairs from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 Calling above evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the uparkot fort of junagarh is especially worth seeing .\n",
      "=            \n",
      "<           <EOS>\n",
      "\n",
      "> be it tug of war camel dance or their acrobatics .\n",
      "=     ,        \n",
      "<  , , ,          <EOS>\n",
      "\n",
      "> save the children from dust .\n",
      "=      \n",
      "<        <EOS>\n",
      "\n",
      "> nearest railway station is kangra temple lrb kms rrb .\n",
      "=      ( 22 .. )  \n",
      "<    -  ( .. .. ) .. )  <EOS>\n",
      "\n",
      "> in bright sunshine above the roofm of houses a thick layer of snow is sparkling .\n",
      "= -               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/aniruddha/.local/lib/python3.5/site-packages/ipykernel_launcher.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<                    \n",
      "\n",
      "> the feeling of yellow color carpet with the sheen of golden mustard in the spring season appear pleasant .\n",
      "=                \n",
      "<                 <EOS>\n",
      "\n",
      "> kangchup is a hilly place which is source of mineral oil .\n",
      "=           \n",
      "<               <EOS>\n",
      "\n",
      "> all the animals eat only those things which is available around them .\n",
      "=             \n",
      "<          , ,     <EOS>\n",
      "\n",
      "> it is estimated that bhagirathji had done his penance here .\n",
      "=    ,         \n",
      "<              <EOS>\n",
      "\n",
      "> mainly only two types of mustards are found red and yellow .\n",
      "=    ,           \n",
      "<                  <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data()\n",
    "attn_model = 'general'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = AttentionDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Load model parameters\n",
    "encoder.load_state_dict(torch.load('../../Models/Eff_Attn_NMT/Encoder/eng_to_hin_encoder_params_{}'.format(attn_model)))\n",
    "decoder.load_state_dict(torch.load('../../Models/Eff_Attn_NMT/Decoder/eng_to_hin_decoder_params_{}'.format(attn_model)))\n",
    "decoder.attention.load_state_dict(torch.load('../../Models/Eff_Attn_NMT/Attention/eng_to_hin_attention_params_{}'.format(attn_model)))\n",
    "\n",
    "# Move models to GPU\n",
    "encoder.cuda()\n",
    "decoder.cuda()\n",
    "\n",
    "evaluateRandomly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
